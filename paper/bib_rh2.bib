
@article{ahmed2021,
  title = {A {{Graph Neural Network Approach}} for {{Product Relationship Prediction}}},
  author = {Ahmed, Faez and Cui, Yaxin and Fu, Yan and Chen, Wei},
  year = {2021},
  journal = {arXiv:2105.05881},
  eprint = {2105.05881},
  eprinttype = {arxiv},
  abstract = {Graph Neural Networks have revolutionized many machine learning tasks in recent years, ranging from drug discovery, recommendation systems, image classification, social network analysis to natural language understanding. This paper shows their efficacy in modeling relationships between products and making predictions for unseen product networks. By representing products as nodes and their relationships as edges of a graph, we show how an inductive graph neural network approach, named GraphSAGE, can efficiently learn continuous representations for nodes and edges. These representations also capture product feature information such as price, brand, or engineering attributes. They are combined with a classification model for predicting the existence of the relationship between products. Using a case study of the Chinese car market, we find that our method yields double the prediction performance compared to an Exponential Random Graph Model-based method for predicting the co-consideration relationship between cars. While a vanilla GraphSAGE requires a partial network to make predictions, we introduce an `adjacency prediction model' to circumvent this limitation. This enables us to predict product relationships when no neighborhood information is known. Finally, we demonstrate how a permutation-based interpretability analysis can provide insights on how design attributes impact the predictions of relationships between products. This work provides a systematic method to predict the relationships between products in many different markets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {\url{http://arxiv.org/abs/2105.05881}},
  file = {D\:\\Bibliography\\ahmed2021.pdf}
}

@article{chen2020,
  title = {{{KNN-DBSCAN}}: A {{DBSCAN}} in High Dimensions},
  shorttitle = {{{KNN-DBSCAN}}},
  author = {Chen, Youguang and Ruys, William and Biros, George},
  year = {2020},
  journal = {arXiv:2009.04552},
  eprint = {2009.04552},
  eprinttype = {arxiv},
  abstract = {Clustering is a fundamental task in machine learning. One of the most successful and broadly used algorithms is DBSCAN, a density-based clustering algorithm. DBSCAN requires -nearest neighbor graphs of the input dataset, which are computed with range-search algorithms and spatial data structures like KD-trees. Despite many efforts to design scalable implementations for DBSCAN, existing work is limited to low-dimensional datasets, as constructing -nearest neighbor graphs is expensive in high-dimensions. In this paper, we modify DBSCAN to enable use of {$\kappa$}-nearest neighbor graphs of the input dataset. The {$\kappa$}-nearest neighbor graphs are constructed using approximate algorithms based on randomized projections. Although these algorithms can become inaccurate or expensive in high-dimensions, they possess a much lower memory overhead than constructing nearest neighbor graphs (O(nk) vs. O(n2)). We delineate the conditions under which kNN-DBSCAN produces the same clustering as DBSCAN. We also present an efficient parallel implementation of the overall algorithm using OpenMP for shared memory and MPI for distributed memory parallelism. We present results on up to 16 billion points in 20 dimensions, and perform weak and strong scaling studies using synthetic data. Our code is efficient in both low and high dimensions. We can cluster one billion points in 3D in less than one second on 28K cores on the Frontera system at the Texas Advanced Computing Center (TACC). In our largest run, we cluster 65 billion points in 20 dimensions in less than 40 seconds using 114,688 x86 cores on TACC's Frontera system. Also, we compare with a state of the art parallel DBSCAN code; on 20d/4M point dataset, our code is up to 37\texttimes{} faster.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  note = {\url{http://arxiv.org/abs/2009.04552}},
  file = {D\:\\Bibliography\\chen2020.pdf}
}

@article{chen2021b,
  title = {{{BLOCK-DBSCAN}}: {{Fast}} Clustering for Large Scale Data},
  shorttitle = {{{BLOCK-DBSCAN}}},
  author = {Chen, Yewang and Zhou, Lida and Bouguila, Nizar and Wang, Cheng and Chen, Yi and Du, Jixiang},
  year = {2021},
  journal = {Pattern Recognition},
  volume = {109},
  pages = {107624},
  issn = {00313203},
  doi = {10.1016/j.patcog.2020.107624},
  langid = {english},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320320304271}},
  file = {D\:\\Bibliography\\chen2021b.pdf}
}

@article{chiong2018,
  title = {Estimation of Graphical Models Using the {{L1}},2 Norm},
  author = {Chiong, Khai Xiang and Moon, Hyungsik Roger},
  year = {2018},
  journal = {The Econometrics Journal},
  volume = {21},
  number = {3},
  pages = {247--263},
  issn = {1368-4221, 1368-423X},
  doi = {10.1111/ectj.12104},
  abstract = {Gaussian graphical models are recently used in economics to obtain networks of dependence among agents. A widely used estimator is the graphical least absolute shrinkage and selection operator (GLASSO), which amounts to a maximum likelihood estimation regularized using the L1,1 matrix norm on the precision matrix . The L1,1 norm is a LASSO penalty that controls for sparsity, or the number of zeros in . We propose a new estimator called structured GLASSO (SGLASSO) that uses the L1,2 mixed norm. The use of the L1,2 penalty controls for the structure of the sparsity in . We show that when the network size is fixed, SGLASSO is asymptotically equivalent to an infeasible GLASSO problem which prioritizes the sparsity-recovery of high-degree nodes. Monte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of estimating the overall precision matrix and in terms of estimating the structure of the graphical model. In an empirical illustration using a classic firms' investment data set, we obtain a network of firms' dependence that exhibits the core\textendash periphery structure, with General Motors, General Electric and US Steel forming the core group of firms.},
  langid = {english},
  note = {\url{https://academic.oup.com/ectj/article/21/3/247/5145987}},
  file = {D\:\\Bibliography\\chiong2018.pdf}
}

@article{cunningham2021,
  title = {K-{{Nearest Neighbour Classifiers}}: 2nd {{Edition}} (with {{Python}} Examples)},
  shorttitle = {K-{{Nearest Neighbour Classifiers}}},
  author = {Cunningham, Padraig and Delany, Sarah Jane},
  year = {2021},
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  eprint = {2004.04523},
  eprinttype = {arxiv},
  pages = {1--25},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3459665},
  abstract = {Perhaps the most straightforward classifier in the arsenal or machine learning techniques is the Nearest Neighbour Classifier \textendash{} classification is achieved by identifying the nearest neighbours to a query example and using those neighbours to determine the class of the query. This approach to classification is of particular importance because issues of poor run-time performance is not such a problem these days with the computational power that is available. This paper presents an overview of techniques for Nearest Neighbour classification focusing on; mechanisms for assessing similarity (distance), computational issues in identifying nearest neighbours and mechanisms for reducing the dimension of the data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {\url{http://arxiv.org/abs/2004.04523}},
  file = {D\:\\Bibliography\\cunningham2021.pdf}
}

@inproceedings{hao2020,
  title = {Inductive {{Link Prediction}} for {{Nodes Having Only Attribute Information}}},
  booktitle = {Proceedings of the 29th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Hao, Yu and Cao, Xin and Fang, Yixiang and Xie, Xike and Wang, Sibo},
  year = {2020},
  eprint = {2007.08053},
  eprinttype = {arxiv},
  pages = {1209--1215},
  doi = {10.24963/ijcai.2020/168},
  abstract = {Predicting the link between two nodes is a fundamental problem for graph data analytics. In attributed graphs, both the structure and attribute information can be utilized for link prediction. Most existing studies focus on transductive link prediction where both nodes are already in the graph. However, many real-world applications require inductive prediction for new nodes having only attribute information. It is more challenging since the new nodes do not have structure information and cannot be seen during the model training. To solve this problem, we propose a model called DEAL, which consists of three components: two node embedding encoders and one alignment mechanism. The two encoders aim to output the attributeoriented node embedding and the structure-oriented node embedding, and the alignment mechanism aligns the two types of embeddings to build the connections between the attributes and links. Our model DEAL is versatile in the sense that it works for both inductive and transductive link prediction. Extensive experiments on several benchmark datasets show that our proposed model significantly outperforms existing inductive link prediction methods, and also outperforms the state-ofthe-art methods on transductive link prediction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  note = {\url{http://arxiv.org/abs/2007.08053}},
  file = {D\:\\Bibliography\\hao2020.pdf}
}

@inproceedings{liu2020a,
  title = {Deep {{Learning}} for {{Community Detection}}: {{Progress}}, {{Challenges}} and {{Opportunities}}},
  shorttitle = {Deep {{Learning}} for {{Community Detection}}},
  booktitle = {29th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liu, Fanzhen and Xue, Shan and Wu, Jia and Zhou, Chuan and Hu, Wenbin and Paris, Cecile and Nepal, Surya and Yang, Jian and Yu, Philip S.},
  year = {2020},
  eprint = {2005.08225},
  eprinttype = {arxiv},
  pages = {4981--4987},
  doi = {10.24963/ijcai.2020/693},
  abstract = {As communities represent similar opinions, similar functions, similar purposes, etc., community detection is an important and extremely useful tool in both scientific inquiry and data analytics. However, the classic methods of community detection, such as spectral clustering and statistical inference, are falling by the wayside as deep learning techniques demonstrate an increasing capacity to handle high-dimensional graph data with impressive performance. Thus, a survey of current progress in community detection through deep learning is timely. Structured into three broad research streams in this domain \textendash{} deep neural networks, deep graph embedding, and graph neural networks, this article summarizes the contributions of the various frameworks, models, and algorithms in each stream along with the current challenges that remain unsolved and the future research opportunities yet to be explored.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  note = {\url{http://arxiv.org/abs/2005.08225}},
  file = {D\:\\Bibliography\\liu2020a2.pdf}
}

@inproceedings{ni2019,
  title = {Justifying {{Recommendations}} Using {{Distantly-Labeled Reviews}} and {{Fine-Grained Aspects}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
  year = {2019},
  pages = {188--197},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1018},
  abstract = {Several recent works have considered the problem of generating reviews (or `tips') as a form of explanation as to why a recommendation might match a user's interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users' decision-making process. We seek to introduce new datasets and methods to address this recommendation justification task. In terms of data, we first propose an `extractive' approach to identify review segments which justify users' intentions; this approach is then used to distantly label massive review corpora and construct largescale personalized recommendation justification datasets. In terms of generation, we design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.},
  langid = {english},
  note = {\url{https://www.aclweb.org/anthology/D19-1018}},
  file = {D\:\\Bibliography\\ni2019.pdf}
}

@article{su2021,
  title = {A {{Comprehensive Survey}} on {{Community Detection}} with {{Deep Learning}}},
  author = {Su, Xing and Xue, Shan and Liu, Fanzhen and Wu, Jia and Yang, Jian and Zhou, Chuan and Hu, Wenbin and Paris, Cecile and Nepal, Surya and Jin, Di and Sheng, Quan Z. and Yu, Philip S.},
  year = {2021},
  journal = {arXiv:2105.12584},
  eprint = {2105.12584},
  eprinttype = {arxiv},
  abstract = {A community reveals the features and connections of its members that are different from those in other communities in a network. Detecting communities is of great significance in network analysis. Despite the classical spectral clustering and statistical inference methods, we notice a significant development of deep learning techniques for community detection in recent years with their advantages in handling high dimensional network data. Hence, a comprehensive overview of community detection's latest progress through deep learning is timely to academics and practitioners. This survey devises and proposes a new taxonomy covering different state-of-the-art methods, including deep learning-based models upon deep neural networks, deep nonnegative matrix factorization and deep sparse filtering. The main category, i.e., deep neural networks, is further divided into convolutional networks, graph attention networks, generative adversarial networks and autoencoders. The survey also summarizes the popular benchmark data sets, evaluation metrics, and open-source implementations to address experimentation settings. We then discuss the practical applications of community detection in various domains and point to implementation scenarios. Finally, we outline future directions by suggesting challenging topics in this fast-growing deep learning field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  note = {\url{http://arxiv.org/abs/2105.12584}},
  file = {D\:\\Bibliography\\su2021.pdf}
}

@article{xia2021,
  title = {Graph {{Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Learning}}},
  author = {Xia, Feng and Sun, Ke and Yu, Shuo and Aziz, Abdul and Wan, Liangtian and Pan, Shirui and Liu, Huan},
  year = {2021},
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {2},
  eprint = {2105.00696},
  eprinttype = {arxiv},
  pages = {109--127},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3076021},
  abstract = {Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,I.2.6},
  note = {\url{http://arxiv.org/abs/2105.00696}},
  file = {D\:\\Bibliography\\xia2021.pdf}
}

@article{xu2021,
  title = {Graph Embedding Clustering: {{Graph}} Attention Auto-Encoder with Cluster-Specificity Distribution},
  shorttitle = {Graph Embedding Clustering},
  author = {Xu, Huiling and Xia, Wei and Gao, Quanxue and Han, Jungong and Gao, Xinbo},
  year = {2021},
  journal = {Neural Networks},
  volume = {142},
  pages = {221--230},
  issn = {08936080},
  doi = {10.1016/j.neunet.2021.05.008},
  langid = {english},
  keywords = {to read},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0893608021002008}},
  file = {D\:\\Bibliography\\xu2021.pdf}
}

@article{zhang2021,
  title = {Spectral Embedding Network for Attributed Graph Clustering},
  author = {Zhang, Xiaotong and Liu, Han and Wu, Xiao-Ming and Zhang, Xianchao and Liu, Xinyue},
  year = {2021},
  journal = {Neural Networks},
  volume = {142},
  pages = {388--396},
  issn = {08936080},
  doi = {10.1016/j.neunet.2021.05.026},
  abstract = {Attributed graph clustering aims to discover node groups by utilizing both graph structure and node features. Recent studies mostly adopt graph neural networks to learn node embeddings, then apply traditional clustering methods to obtain clusters. However, they usually suffer from the following issues: (1) they adopt original graph structure which is unfavorable for clustering due to its noise and sparsity problems; (2) they mainly utilize non-clustering driven losses that cannot well capture the global cluster structure, thus the learned embeddings are not sufficient for the downstream clustering task. In this paper, we propose a spectral embedding network for attributed graph clustering (SENet), which improves graph structure by leveraging the information of shared neighbors, and learns node embeddings with the help of a spectral clustering loss. By combining the original graph structure and shared neighbor based similarity, both the first-order and second-order proximities are encoded into the improved graph structure, thus alleviating the noise and sparsity issues. To make the spectral loss well adapt to attributed graphs, we integrate both structure and feature information into kernel matrix via a higher-order graph convolution. Experiments on benchmark attributed graphs show that SENet achieves superior performance over state-of-the-art methods.},
  langid = {english},
  keywords = {to read},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0893608021002227}},
  file = {D\:\\Bibliography\\zhang2021.pdf}
}

@article{zhao2009,
  title = {The Composite Absolute Penalties Family for Grouped and Hierarchical Variable Selection},
  author = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
  year = {2009},
  journal = {The Annals of Statistics},
  volume = {37},
  number = {6A},
  eprint = {0909.0411},
  eprinttype = {arxiv},
  issn = {0090-5364},
  doi = {10.1214/07-AOS584},
  abstract = {Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the \$L\_1\$-penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including \$L\_1\$ to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the BLASSO and cross-validation to compute CAP estimates in general. For a subfamily of CAP estimates involving only the \$L\_1\$ and \$L\_\{\textbackslash infty\}\$ norms, we introduce the iCAP algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. CAP is shown to improve on the predictive performance of the LASSO in a series of simulated experiments, including cases with \$p\textbackslash gg n\$ and possibly mis-specified groupings. When the complexity of a model is properly calculated, iCAP is seen to be parsimonious in the experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62J07 (Primary),Mathematics - Statistics Theory,to read},
  note = {\url{http://arxiv.org/abs/0909.0411}},
  file = {D\:\\Bibliography\\zhao2009.pdf}
}

@article{zhou2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.01.001},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
  langid = {english},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012}},
  file = {D\:\\Bibliography\\zhou2020.pdf}
}


